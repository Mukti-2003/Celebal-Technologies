# -*- coding: utf-8 -*-
"""ASSIGNMENT-08_WEEK-08.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/105hlqSvsK6W7GwAKt5C_Oc9vy4AIO6U1
"""

# RAG Q&A Chatbot: Loan Approval Dataset

# ==============================
# 1. Install and Import Libraries
# ==============================
!pip install faiss-cpu transformers datasets sentence-transformers --quiet

!pip install faiss-cpu openai sentence-transformers --quiet

import pandas as pd
import numpy as np
import faiss
import torch
import openai
from sentence_transformers import SentenceTransformer
from IPython.display import display

# 2. Load Uploaded Datasets
# ==============================
train_path = "/content/Training Dataset.csv"
test_path = "/content/Test Dataset.csv"
sample_path = "/content/Sample_Submission.csv"

train_df = pd.read_csv(train_path)
test_df = pd.read_csv(test_path)
sample_df = pd.read_csv(sample_path)

# Clean training data
train_df.dropna(inplace=True)
train_df.reset_index(drop=True, inplace=True)
print("Training Data Loaded! Shape:", train_df.shape)
display(train_df.head())

# 3. Convert Training Rows into Text Chunks
# ==============================
corpus = []
for idx, row in train_df.iterrows():
    text = f"Applicant {idx+1}: Gender: {row['Gender']}, Married: {row['Married']}, Education: {row['Education']}, Self_Employed: {row['Self_Employed']}, ApplicantIncome: {row['ApplicantIncome']}, LoanAmount: {row['LoanAmount']}, Credit_History: {row['Credit_History']}, Property_Area: {row['Property_Area']}, Loan_Status: {row['Loan_Status']}"
    corpus.append(text)

print("Sample document:", corpus[0])

# ==============================
# 4. Create Embeddings and Vector Store (FAISS)
# ==============================
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(corpus, convert_to_tensor=True)
embeddings_np = embeddings.detach().cpu().numpy()

# Create FAISS Index
dimension = embeddings_np.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(embeddings_np)
print("FAISS index created with", index.ntotal, "documents.")

from openai import OpenAI
client = OpenAI(api_key="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")

openai.api_key = "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"

def rag_answer(query, top_k=3):
    query_embedding = embedding_model.encode([query], convert_to_tensor=True).detach().cpu().numpy()
    D, I = index.search(query_embedding, top_k)
    context = "\n".join([corpus[i] for i in I[0]])
    prompt = f"Answer the following question using the context below:\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer:"

    try:
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that answers questions based on provided context."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=200
        )
        return response['choices'][0]['message']['content'].strip()
    except Exception as e:
        return f"Error during OpenAI API call: {e}"

print("\nAsk questions about loan approvals. Examples:")
print("- What affects loan approval?\n- How does income impact loan amount?\n")

query = "What affects loan approval?"
response = rag_answer(query)
print("Q:", query)
print("A:", response)

query2 = "How does education affect loan approval?"
response2 = rag_answer(query2)
print("\nQ:", query2)
print("A:", response2)

print("\nâœ… RAG chatbot is working with OpenAI GPT and the training dataset. You can expand it to handle test predictions or create a UI using Streamlit or Gradio.")

